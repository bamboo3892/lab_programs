# -*- coding: utf-8 -*-
"""
Created on SEP 19 2018

@author: takeda masaki
"""


import numpy as np
import json


class ASUM:

    DEFAULT_BETA = [
        [0.01, 0.01, 0.00001],
        [0.00001, 0.01, 0.01]
    ]

    def __init__(self, K, J, alpha, beta, gamma, docs, dictionary):
        """
        K         : number of topics\n
        J         : number of sentiments\n
        alpha     : parameter of theta\n
                    same value in documents, sentiments and topics\n
        beta      : parameter of phi\n
                    varies depending on sentence's sentiment and word's eigen sentiment\n
                    [sentence's sentiment, word's sentiment]
                    Sugested values:
                    [
                        [0.01, 0.01, 0.00001],
                        [0.00001, 0.01, 0.01]
                    ]
        gamma     : parameter of pi\n
                    same value in documents
        docs      : documents\n
                    [
                        [
                            [
                                [word], [word], ...
                            ], [sentence], [sentence], ...
                        ], [document], [document], ...
                    ]
        dictionary: sentiment dictionary\n
                    [
                        "word": 1(pos) or -1(neg),
                        "word": 1(pos) or -1(neg),
                        ...
                    ]
        """

        """ 1. init variables """
        # init learing materials
        self.docs = docs
        self.dict = dictionary
        self.words = []
        # ["word", "word", ...]
        self.sentiments = []
        # [1, 0, -1, ...]
        self.nWord = []
        # [
        #     [    <-document
        #         [    <-sentence
        #             [1, 2, 3, ...]
        #         ], [], ...
        #     ], [], ...
        # ]
        for document in self.docs:
            for sentence in document:
                for word in sentence:
                    if(word not in self.words):
                        self.words.append(word)
                        if(word in self.dict):
                            self.sentiments.append(self.dict[word])
                        else:
                            self.sentiments.append(0)
        D = len(self.docs)
        V = len(self.words)
        for document in self.docs:
            l0 = []
            for sentence in document:
                l1 = [0] * V
                for word in sentence:
                    l1[self.words.index(word)] += 1
                l0.append(l1)
            self.nWord.append(l0)

        # init fields
        self.D = D  # number of documents
        self.L = []  # number of sentences of each document
        self.N = []  # number of words of each sentence
        self.V = V  # number of word in all documents
        self.J = J  # number of sentiments
        self.K = K  # number of topics
        for document in self.docs:
            self.L.append(len(document))
            l2 = []
            for sentence in document:
                l2.append(len(sentence))
            self.N.append(l2)

        # init parameters of each prior probability
        self.gamma = np.full(J, gamma)  # parameter of pi
        self.alpha = np.full(K, alpha)  # parameter of theta
        self.beta = np.zeros((V, J))  # parameter of phi
        for v in range(0, self.V):
            if self.sentiments[v] == 1:
                self.beta[v, 1] = beta[0][0]
                self.beta[v, 0] = beta[1][0]
            if self.sentiments[v] == 0:
                self.beta[v, 1] = beta[0][1]
                self.beta[v, 0] = beta[1][1]
            if self.sentiments[v] == -1:
                self.beta[v, 1] = beta[0][2]
                self.beta[v, 0] = beta[1][2]

        # init prior probability
        self.pi = np.zeros((D, J))  # sentence's sentiments prior probability for each document
        self.theta = np.zeros((D, J, K))  # sentence's topic prior probability for each document, given sentence's sentiment
        self.phi = np.zeros((J, K, V))  # words prior probability given sentence's sentiment and topic

        # init latent variables
        self.s = []  # sentence's sentiments, generated by pi
        self.z = []  # sentence's topic, generated by theta
        for d in range(0, self.D):
            self.s.append([0] * self.L[d])
            self.z.append([0] * self.L[d])

        # init counts
        self.n_m_j_z = np.zeros((D, J, K))  # count of sentences, given d, j, k
        self.n_m_j = np.zeros((D, J))  # count of sentences, given d, j
        self.n_j_z_t = np.zeros((J, K, V))  # count of words, given j, k, v
        self.n_j_z = np.zeros((J, K))  # count of words, given j, k

        """ 2. sampleing initial values of s, z """
        for d, document in enumerate(self.nWord):

            # sampling topic z and sentiment s
            for l, sentence in enumerate(document):
                p_j_z = np.zeros((self.J, self.K))  # s=j, z=kの確率
                C_dj = (self.gamma) / (self.gamma.sum())  # gammaの縦ベクトル J*1
                C_djk = (self.alpha).reshape(1, self.K) / (self.alpha.sum())  # alphaの横ベクトル 1*K
                C_jkw_1 = 1  # betaのなんか single value
                for v, count in enumerate(sentence):
                    C_jkw_1 *= np.power(self.beta[v].reshape(self.J, 1) / (np.sum(self.beta, axis=0).reshape(self.J, 1)), count)
                p_j_z = C_dj.reshape([self.J, 1]) * C_djk * C_jkw_1
                p_j_zST = p_j_z.reshape(-1)
                sz = np.random.multinomial(1, p_j_zST / p_j_zST.sum()).argmax()
                s0 = sz // self.K
                z0 = sz % self.K
                self.z[d][l] = z0
                self.s[d][l] = s0

                # update counts
                for v, count in enumerate(sentence):
                    self.n_j_z_t[s0, z0, v] += count
                    self.n_j_z[s0, z0] += count
                self.n_m_j[d, s0] += 1
                self.n_m_j_z[d, s0, z0] += 1

            # sampling pi for document "d"
            self.pi[d, :] = (self.n_m_j[d, :] + self.gamma) / (np.sum(self.n_m_j, axis=1)[d] + self.gamma.sum())

            # sampling theta for document "d"
            self.theta[d] = (self.n_m_j_z[d] + self.alpha) / (np.sum(self.n_m_j_z, axis=2)[d] + np.sum(self.alpha)).reshape([self.J, 1])

        # sampling Phi for topic "k"
        self.phi = (self.n_j_z_t + self.beta.transpose((1, 0)).reshape([self.J, 1, self.V])) / (np.sum(self.n_j_z_t, axis=2) + np.sum(self.beta, axis=0).reshape(self.J, 1)).reshape(self.J, self.K, 1)

    def learn(self):
        """ Execute single learning loop """
        for d, document in enumerate(self.nWord):
            n_m_j = self.n_m_j[d]
            n_m_j_z = self.n_m_j_z[d]  # 文書Mにおける各トピックの出現回数

            for l, sentence in enumerate(document):
                s = self.s[d][l]
                z = self.z[d][l]
                n_m_j[s] -= 1
                n_m_j_z[s, z] -= 1

                for v, count in enumerate(sentence):
                    self.n_j_z_t[s, z, v] -= count
                    self.n_j_z[s, z] -= count

                p_j_z = np.zeros((self.J, self.K))
                p_jlist = np.zeros(self.J)
                p_zlist = np.zeros(self.K)

                # sampling topic new_z  and sentiment new_j
                SUM1 = np.sum(n_m_j_z, axis=1)
                SUM2 = np.sum(SUM1)
                C_dj = (SUM1 + self.gamma) / (SUM2 + self.gamma.sum())
                C_djk = (n_m_j_z + self.alpha) / (np.sum(n_m_j_z, axis=1) + self.alpha.sum()).reshape([self.J, 1])

                C_jkw_1 = 1
                for v, count in enumerate(sentence):
                    C_jkw_1 *= np.power((self.n_j_z_t[:, :, v] + self.beta[v].reshape([self.J, 1])) / (self.n_j_z + np.sum(self.beta, axis=0).reshape([self.J, 1])), 2)
                p_j_z = C_dj.reshape([self.J, 1]) * C_djk * C_jkw_1
                p_j_zST = p_j_z.reshape(-1)
                sz = np.random.multinomial(1, p_j_zST / p_j_zST.sum()).argmax()
                new_j = sz // self.K
                new_z = sz % self.K
                self.z[d][l] = new_z
                self.s[d][l] = new_j

                # update counts
                n_m_j_z[new_j, new_z] += 1
                n_m_j[new_j] += 1
                for v, count in enumerate(sentence):
                    self.n_j_z_t[new_j, new_z, v] += count
                    self.n_j_z[new_j, new_z] += count

            # pi, theta, phiは最後に一回求めれば良くない？ #################################
            # sampling pie for document "d"
            self.pi[d, :] = (self.n_m_j[d, :] + self.gamma) / (np.sum(self.n_m_j, axis=1)[d] + self.gamma.sum())

            # sampling theta for document "d"
            self.theta[d] = (self.n_m_j_z[d] + self.alpha) / (np.sum(self.n_m_j_z, axis=2)[d] + np.sum(self.alpha)).reshape([self.J, 1])

        # sampling Phi for topic "k"
        self.phi = (self.n_j_z_t + self.beta.transpose((1, 0)).reshape([self.J, 1, self.V])) / (np.sum(self.n_j_z_t, axis=2) + np.sum(self.beta, axis=0).reshape(self.J, 1)).reshape(self.J, self.K, 1)

    def startLearning(self, loop, showLog=False):
        """ Start leaning loops """
        for i in range(loop):
            self.learn()
            if(showLog):
                print(i)


def testASUM(pathMorphomes, pathDictionary, pathResultFolder):
    """ Example usage for ASUM class. """
    docs = []
    dictionary = []
    with open(pathMorphomes, "r", encoding="utf8") as f0:
        docs = json.load(f0)
    with open(pathDictionary, "r", encoding="utf8") as f1:
        dictionary = json.load(f1)

    model = ASUM(5, 2, 0.01, ASUM.DEFAULT_BETA, 0.01, docs, dictionary)
    model.startLearning(500, showLog=True)
    print("")
